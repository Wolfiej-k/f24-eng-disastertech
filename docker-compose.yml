services:
  app:
    build:
      context: .
      dockerfile: app.Dockerfile
    container_name: frontend
    environment:
      - NEXT_PUBLIC_URL=http://localhost:3000
      - API_PORT=4000
    ports:
      - "3000:3000"
    volumes:
      - ./src/app:/app/src/app
      - ./src/components:/app/src/components
      - ./src/hooks:/app/src/hooks
      - ./src/lib:/app/src/lib
    depends_on:
      - api

  api:
    build:
      context: .
      dockerfile: api.Dockerfile
    container_name: backend
    environment:
      - POSTGRES_USER=t4sg
      - POSTGRES_PASSWORD=disaster-tech-lab
      - POSTGRES_DB=offline-ai
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - API_PORT=4000
      - AUTH_SECRET=${AUTH_SECRET}
      - ADMIN_PASSWORD=${ADMIN_PASSWORD}
      - TOP_DOCUMENTS=5
      - TOP_HISTORY=10
      - MAX_TOKENS=128
    volumes:
      - ./src/api:/api/src/api
      - /var/run/docker.sock:/var/run/docker.sock
    depends_on:
      - postgres
      - llama

  # https://github.com/ggerganov/llama.cpp/tree/master/examples/server#usage
  llama:
    image: ghcr.io/ggerganov/llama.cpp:server
    container_name: llama
    environment:
      - LLAMA_ARG_MODEL_URL=https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q3_K_L.gguf
      - LLAMA_ARG_MODEL=/models/llama.gguf
      - LLAMA_ARG_CTX_SIZE=4096
      - LLAMA_N_BATCH=512
      - LLAMA_N_CTX=2048
      - LLAMA_ARG_PORT=8080
      - LLAMA_ARG_METRICS=1
    command: ["--metrics"]
    volumes:
      - models:/models

  # https://github.com/pgvector/pgvector?tab=readme-ov-file#additional-installation-methods
  postgres:
    image: pgvector/pgvector:pg17
    container_name: database
    environment:
      - POSTGRES_PASSWORD=disaster-tech-lab
      - POSTGRES_DB=offline-ai
      - POSTGRES_HOST=postgres
    volumes:
      - pgdata:/var/lib/postgresql/data
      - ./setup.sql:/docker-entrypoint-initdb.d/setup.sql

volumes:
  models:
  pgdata:
